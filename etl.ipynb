{ "cells": [  {   "metadata": {},   "cell_type": "code",   "source": "!pip install chardet",   "id": "38ebf8bee66f072c",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Dictionary of columns from NMT and ZNO",   "id": "3d531aa36bb7e8fe"  },  {   "cell_type": "code",   "id": "initial_id",   "metadata": {    "collapsed": true   },   "source": [    "config = {\n",    "    \"NMT\": {\n",    "        \"outid\": \"id\",\n",    "        \"Birth\": \"birth_year\",\n",    "        \"SexTypeName\": \"gender\",\n",    "        \"RegName\": \"region_name\",\n",    "        \"AreaName\": \"area_name\",\n",    "        \"TerName\": \"territory_name\",\n",    "        \"RegTypeName\": \"region_type\",\n",    "        \"TerTypeName\": \"territory_type\",\n",    "        \"EOName\": \"education_org_name\",\n",    "        \"EOTypeName\": \"education_org_type\",\n",    "        \"EORegName\": \"education_org_region\",\n",    "        \"EOAreaName\": \"education_org_area\",\n",    "        \"EOTerName\": \"education_org_territory\",\n",    "        \"EOParent\": \"education_org_parent\",\n",    "        \"Test\": \"test_name\",\n",    "        \"TestDate\": \"test_date\",\n",    "        \n",    "        \"UkrBlock\": \"ukrainian_test_name\",\n",    "        \"UkrBlockStatus\": \"ukrainian_test_status\",\n",    "        \"UkrBlockBall100\": \"ukrainian_score_100\",\n",    "        \"UkrBlockBall\": \"ukrainian_score\",\n",    "\n",    "        \"HistBlock\": \"history_test_name\",\n",    "        \"HistBlockLang\": \"history_language\",\n",    "        \"HistBlockStatus\": \"history_test_status\",\n",    "        \"HistBlockBall100\": \"history_score_100\",\n",    "        \"HistBlockBall\": \"history_score\",\n",    "\n",    "        \"MathBlock\": \"math_test_name\",\n",    "        \"MathBlockLang\": \"math_language\",\n",    "        \"MathBlockStatus\": \"math_test_status\",\n",    "        \"MathBlockBall100\": \"math_score_100\",\n",    "        \"MathBlockBall\": \"math_score\",\n",    "\n",    "        \"PhysBlock\": \"physics_test_name\",\n",    "        \"PhysBlockLang\": \"physics_language\",\n",    "        \"PhysBlockStatus\": \"physics_test_status\",\n",    "        \"PhysBlockBall100\": \"physics_score_100\",\n",    "        \"PhysBlockBall\": \"physics_score\",\n",    "\n",    "        \"ChemBlock\": \"chemistry_test_name\",\n",    "        \"ChemBlockLang\": \"chemistry_language\",\n",    "        \"ChemBlockStatus\": \"chemistry_test_status\",\n",    "        \"ChemBlockBall100\": \"chemistry_score_100\",\n",    "        \"ChemBlockBall\": \"chemistry_score\",\n",    "\n",    "        \"BioBlock\": \"biology_test_name\",\n",    "        \"BioBlockLang\": \"biology_language\",\n",    "        \"BioBlockStatus\": \"biology_test_status\",\n",    "        \"BioBlockBall100\": \"biology_score_100\",\n",    "        \"BioBlockBall\": \"biology_score\",\n",    "\n",    "        \"GeoBlock\": \"geography_test_name\",\n",    "        \"GeoBlockLang\": \"geography_language\",\n",    "        \"GeoBlockStatus\": \"geography_test_status\",\n",    "        \"GeoBlockBall100\": \"geography_score_100\",\n",    "        \"GeoBlockBall\": \"geography_score\",\n",    "\n",    "        \"EngBlock\": \"english_test_name\",\n",    "        \"EngBlockStatus\": \"english_test_status\",\n",    "        \"EngBlockBall100\": \"english_score_100\",\n",    "        \"EngBlockBall\": \"english_score\",\n",    "\n",    "        \"FraBlock\": \"french_test_name\",\n",    "        \"FraBlockStatus\": \"french_test_status\",\n",    "        \"FraBlockBall100\": \"french_score_100\",\n",    "        \"FraBlockBall\": \"french_score\",\n",    "\n",    "        \"DeuBlock\": \"german_test_name\",\n",    "        \"DeuBlockStatus\": \"german_test_status\",\n",    "        \"DeuBlockBall100\": \"german_score_100\",\n",    "        \"DeuBlockBall\": \"german_score\",\n",    "\n",    "        \"SpaBlock\": \"spanish_test_name\",\n",    "        \"SpaBlockStatus\": \"spanish_test_status\",\n",    "        \"SpaBlockBall100\": \"spanish_score_100\",\n",    "        \"SpaBlockBall\": \"spanish_score\",\n",    "\n",    "        \"UkrLitBlock\": \"ukrainian_lit_test_name\",\n",    "        \"UkrLitBlockStatus\": \"ukrainian_lit_test_status\",\n",    "        \"UkrLitBlockBall100\": \"ukrainian_lit_score_100\",\n",    "        \"UkrLitBlockBall\": \"ukrainian_lit_score\",\n",    "\n",    "        \"PTRegName\": \"pt_region_name\",\n",    "        \"PTAreaName\": \"pt_area_name\",\n",    "        \"PTTerName\": \"pt_territory_name\"\n",    "    },\n",    "\n",    "    \"ZNO\": {\n",    "        \"OUTID\": \"id\",\n",    "        \"Birth\": \"birth_year\",\n",    "        \"SexTypeName\": \"gender\",\n",    "        \"RegName\": \"region_name\",\n",    "        \"AREANAME\": \"area_name\",\n",    "        \"TERNAME\": \"territory_name\",\n",    "        \"RegTypeName\": \"region_type\",\n",    "        \"TerTypeName\": \"territory_type\",\n",    "        \"ClassProfileNAME\": \"class_profile\",\n",    "        \"ClassLangName\": \"class_language\",\n",    "        \"EONAME\": \"education_org_name\",\n",    "        \"EOTypeName\": \"education_org_type\",\n",    "        \"EORegName\": \"education_org_region\",\n",    "        \"EOAreaName\": \"education_org_area\",\n",    "        \"EOTerName\": \"education_org_territory\",\n",    "        \"EOParent\": \"education_org_parent\",\n",    "\n",    "        \"UkrTest\": \"ukrainian_test_name\",\n",    "        \"UkrSubTest\": \"ukrainian_subtest\",\n",    "        \"UkrTestStatus\": \"ukrainian_test_status\",\n",    "        \"UkrBall100\": \"ukrainian_score_100\",\n",    "        \"UkrBall12\": \"ukrainian_score_12\",\n",    "        \"UkrBall\": \"ukrainian_score\",\n",    "        \"UkrAdaptScale\": \"ukrainian_adapt_scale\",\n",    "        \"UkrPTName\": \"ukrainian_pt_name\",\n",    "        \"UkrPTRegName\": \"ukrainian_pt_region\",\n",    "        \"UkrPTAreaName\": \"ukrainian_pt_area\",\n",    "        \"UkrPTTerName\": \"ukrainian_pt_territory\",\n",    "\n",    "        \"HistTest\": \"history_test_name\",\n",    "        \"HistLang\": \"history_language\",\n",    "        \"HistTestStatus\": \"history_test_status\",\n",    "        \"HistBall100\": \"history_score_100\",\n",    "        \"HistBall12\": \"history_score_12\",\n",    "        \"HistBall\": \"history_score\",\n",    "        \"HistPTName\": \"history_pt_name\",\n",    "        \"HistPTRegName\": \"history_pt_region\",\n",    "        \"HistPTAreaName\": \"history_pt_area\",\n",    "        \"HistPTTerName\": \"history_pt_territory\",\n",    "\n",    "        \"MathTest\": \"math_test_name\",\n",    "        \"MathLang\": \"math_language\",\n",    "        \"MathTestStatus\": \"math_test_status\",\n",    "        \"MathBall100\": \"math_score_100\",\n",    "        \"MathBall12\": \"math_score_12\",\n",    "        \"MathDpaLevel\": \"math_dpa_level\",\n",    "        \"MathBall\": \"math_score\",\n",    "        \"MathPTName\": \"math_pt_name\",\n",    "        \"MathPTRegName\": \"math_pt_region\",\n",    "        \"MathPTAreaName\": \"math_pt_area\",\n",    "        \"MathPTTerName\": \"math_pt_territory\",\n",    "\n",    "        \"MathStTest\": \"math_st_test_name\",\n",    "        \"MathStLang\": \"math_st_language\",\n",    "        \"MathStTestStatus\": \"math_st_test_status\",\n",    "        \"MathStBall12\": \"math_st_score_12\",\n",    "        \"MathStBall\": \"math_st_score\",\n",    "        \"MathStPTName\": \"math_st_pt_name\",\n",    "        \"MathStPTRegName\": \"math_st_pt_region\",\n",    "        \"MathStPTAreaName\": \"math_st_pt_area\",\n",    "        \"MathStPTTerName\": \"math_st_pt_territory\",\n",    "\n",    "        \"PhysTest\": \"physics_test_name\",\n",    "        \"PhysLang\": \"physics_language\",\n",    "        \"PhysTestStatus\": \"physics_test_status\",\n",    "        \"PhysBall100\": \"physics_score_100\",\n",    "        \"PhysBall12\": \"physics_score_12\",\n",    "        \"PhysBall\": \"physics_score\",\n",    "        \"PhysPTName\": \"physics_pt_name\",\n",    "        \"PhysPTRegName\": \"physics_pt_region\",\n",    "        \"PhysPTAreaName\": \"physics_pt_area\",\n",    "        \"PhysPTTerName\": \"physics_pt_territory\",\n",    "\n",    "        \"ChemTest\": \"chemistry_test_name\",\n",    "        \"ChemLang\": \"chemistry_language\",\n",    "        \"ChemTestStatus\": \"chemistry_test_status\",\n",    "        \"ChemBall100\": \"chemistry_score_100\",\n",    "        \"ChemBall12\": \"chemistry_score_12\",\n",    "        \"ChemBall\": \"chemistry_score\",\n",    "        \"ChemPTName\": \"chemistry_pt_name\",\n",    "        \"ChemPTRegName\": \"chemistry_pt_region\",\n",    "        \"ChemPTAreaName\": \"chemistry_pt_area\",\n",    "        \"ChemPTTerName\": \"chemistry_pt_territory\",\n",    "\n",    "        \"BioTest\": \"biology_test_name\",\n",    "        \"BioLang\": \"biology_language\",\n",    "        \"BioTestStatus\": \"biology_test_status\",\n",    "        \"BioBall100\": \"biology_score_100\",\n",    "        \"BioBall12\": \"biology_score_12\",\n",    "        \"BioBall\": \"biology_score\",\n",    "        \"BioPTName\": \"biology_pt_name\",\n",    "        \"BioPTRegName\": \"biology_pt_region\",\n",    "        \"BioPTAreaName\": \"biology_pt_area\",\n",    "        \"BioPTTerName\": \"biology_pt_territory\",\n",    "\n",    "        \"GeoTest\": \"geography_test_name\",\n",    "        \"GeoLang\": \"geography_language\",\n",    "        \"GeoTestStatus\": \"geography_test_status\",\n",    "        \"GeoBall100\": \"geography_score_100\",\n",    "        \"GeoBall12\": \"geography_score_12\",\n",    "        \"GeoBall\": \"geography_score\",\n",    "        \"GeoPTName\": \"geography_pt_name\",\n",    "        \"GeoPTRegName\": \"geography_pt_region\",\n",    "        \"GeoPTAreaName\": \"geography_pt_area\",\n",    "        \"GeoPTTerName\": \"geography_pt_territory\",\n",    "                                        \n",    "        \"EngTest\": \"english_test_name\",\n",    "        \"EngTestStatus\": \"english_test_status\",\n",    "        \"EngBall100\": \"english_score_100\",\n",    "        \"EngBall12\": \"english_score_12\",\n",    "        \"EngDPALevel\": \"english_dpa_level\",\n",    "        \"EngBall\": \"english_score\",\n",    "        \"EngPTName\": \"english_pt_name\",\n",    "        \"EngPTRegName\": \"english_pt_region\",\n",    "        \"EngPTAreaName\": \"english_pt_area\",\n",    "        \"EngPTTerName\": \"english_pt_territory\",\n",    "\n",    "        \"FraTest\": \"french_test_name\",\n",    "        \"FraTestStatus\": \"french_test_status\", \n",    "        \"FraBall100\": \"french_score_100\", \n",    "        \"FraBall12\": \"french_score_12\", \n",    "        \"FraDPALevel\": \"french_dpa_level\",    \n",    "        \"FraBall\": \"french_score\",\n",    "        \"FraPTName\": \"french_pt_name\", \n",    "        \"FraPTRegName\":\"french_pt_region\", \n",    "        \"FraPTAreaName\": \"french_pt_area\", \n",    "        \"FraPTTerName\": \"french_pt_territory\", \n",    "\n",    "        \"DeuTest\": \"german_test_name\",\n",    "        \"DeuTestStatus\": \"german_test_status\",\n",    "        \"DeuBall100\": \"german_score_100\",\n",    "        \"DeuBall12\": \"german_score_12\",\n",    "        \"DeuDPALevel\": \"german_dpa_level\",\n",    "        \"DeuBall\": \"german_score\",\n",    "        \"DeuPTName\": \"german_pt_name\", \n",    "        \"DeuPTRegName\": \"german_pt_region\",\n",    "        \"DeuPTAreaName\": \"german_pt_area\", \n",    "        \"DeuPTTerName\": \"german_pt_territory\",\n",    "        \n",    "        \"SpaTest\": \"spanish_test_name\",\n",    "        \"SpaTestStatus\": \"spanish_est_status\", \n",    "        \"SpaBall100\": \"spanish_score_100\", \n",    "        \"SpaBall12\": \"spanish_score_12\", \n",    "        \"SpaDPALevel\": \"spanish_dpa_level\", \n",    "        \"SpaBall\": \"spanish_score\",\n",    "        \"SpaPTName\":\"spanish_pt_name\", \n",    "        \"SpaPTRegName\": \"spanish_pt_region\", \n",    "        \"SpaPTAreaName\": \"spanish_pt_area\",\n",    "        \"SpaPTTerName\": \"spanish_pt_territory\",\n",    "    }\n",    "}\n"   ],   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Preprocess of renaming columns in the 2022 year",   "id": "641f564b98a78710"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import os\n",    "import pandas as pd\n",    "import chardet\n",    "\n",    "file_2022_path = os.path.join(\"data\", \"2022.csv\")\n",    "\n",    "def detect_encoding(file_path):\n",    "    with open(file_path, 'rb') as f:\n",    "        return chardet.detect(f.read(10000))['encoding'] or 'windows-1251'\n",    "\n",    "if os.path.exists(file_2022_path):\n",    "    try:\n",    "        print(f\"üîÑ –û–±—Ä–æ–±–∫–∞ 2022.csv...\")\n",    "        encoding = detect_encoding(file_2022_path)\n",    "        df = pd.read_csv(file_2022_path, encoding=encoding, sep=';', quotechar='\"', on_bad_lines='skip')\n",    "        print(f\"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} —Ä—è–¥–∫—ñ–≤, {len(df.columns)} —Å—Ç–æ–≤–ø—Ü—ñ–≤\")\n",    "\n",    "        rename_columns = {\n",    "            'Block1': 'UkrBlock', 'Block1Ball100': 'UkrBlockBall100', 'Block1Ball': 'UkrBlockBall',\n",    "            'Block2': 'HistBlock', 'Block2Ball100': 'HistBlockBall100', 'Block2Ball': 'HistBlockBall',\n",    "            'Block3': 'MathBlock', 'Block3Ball100': 'MathBlockBall100', 'Block3Ball': 'MathBlockBall'\n",    "        }\n",    "        df.rename(columns=rename_columns, inplace=True)\n",    "        print(f\"   –ü–µ—Ä–µ–π–º–µ–Ω–æ–≤–∞–Ω–æ {len([col for col in rename_columns if col in df.columns])} –∫–æ–ª–æ–Ω–æ–∫\")\n",    "\n",    "        if 'TestStatus' in df.columns:\n",    "            df['UkrBlockStatus'] = df['HistBlockStatus'] = df['MathBlockStatus'] = df['TestStatus']\n",    "            df.drop(columns=['TestStatus'], inplace=True)\n",    "            print(f\"   –î–æ–¥–∞–Ω–æ —Å—Ç–∞—Ç—É—Å–∏, –≤–∏–¥–∞–ª–µ–Ω–æ TestStatus\")\n",    "        else:\n",    "            print(f\"‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ TestStatus –≤—ñ–¥—Å—É—Ç–Ω—è\")\n",    "\n",    "        df.to_csv(file_2022_path, index=False, encoding='utf-8', sep=';', quotechar='\"')\n",    "        print(f\"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –¥–æ {file_2022_path}\")\n",    "\n",    "    except Exception as e:\n",    "        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}\")\n",    "else:\n",    "    print(f\"‚ö†Ô∏è –§–∞–π–ª 2022.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ.\")\n",    "\n",    "print(\"üéâ –û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!\")"   ],   "id": "c7934afb5f5f135c",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "I decided to write separate code for `2022.csv` because this dataset had a unique structure that differed from other NMT datasets (2023, 2024), having only 29 columns. In particular, `2022. csv` had columns with different names, such as `Block1` and `Block1Ball100`, instead of the standard `UkrBlock` or `UkrBlockBall100`, as well as one general column `TestStatus` instead of separate statuses for subjects (`UkrBlockStatus`, `HistBlockStatus`, `MathBlockStatus`). Because of this, `2022.csv` was not recognised as NMT in `transform_data.py`, as it did not reach the 80% match threshold with `config[‚ÄòNMT‚Äô]`. Separate code allowed the structure of `2022.csv` to be unified for further processing.\n",    "\n",    "**Solution:**\n",    "- Created separate block code, which:\n",    "- Renames columns (e.g., `Block1` ‚Üí `UkrBlock`, `Block1Ball100` ‚Üí `UkrBlockBall100`).\n",    "  - Adds columns `UkrBlockStatus`, `HistBlockStatus`, `MathBlockStatus`, copying values from `TestStatus`.\n",    "  - Deletes `TestStatus` for consistency.\n",    "  - Saves the file with a separator `;` and escaping `\"`.\n",    "\n",    "This ensured the correct processing of `2022.csv` as an NMT dataset, unifying it with other files.\n"   ],   "id": "dfc6435d8ccddbee"  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Renaming the columns in all datasets",   "id": "cd9099c1d700da17"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import pandas as pd\n",    "import os\n",    "import json\n",    "import csv\n",    "import chardet\n",    "\n",    "input_folder = \"data\"\n",    "output_folder = \"transformed_data\"\n",    "os.makedirs(output_folder, exist_ok=True)\n",    "\n",    "def detect_mapping(df):\n",    "    df_columns_lower = [col.lower() for col in df.columns]\n",    "\n",    "    for dataset_type in ['NMT', 'ZNO']:\n",    "        if dataset_type in config:\n",    "            mapping = config[dataset_type]\n",    "            mapping_cols_lower = [col.lower() for col in mapping.keys()]\n",    "            matching_cols = [col for col in mapping_cols_lower if col in df_columns_lower]\n",    "            if len(matching_cols) / len(mapping_cols_lower) >= 0.4: # —Ü–µ –≤—ñ–¥—Å–æ—Ç–æ–∫ –∑–±—ñ–≥—É –≤ –∫–æ–ª–æ–Ω–æ–∫ –≤ config (–±—É–ª–æ –∑–º–µ–Ω—à–∞–Ω–æ –∑ 80% –¥–æ 40%)\n",    "                return dataset_type, mapping\n",    "    return None, None\n",    "\n",    "def detect_encoding(file_path):\n",    "    with open(file_path, 'rb') as f:\n",    "        sample = f.read(10000)\n",    "        result = chardet.detect(sample)\n",    "        return result['encoding']\n",    "\n",    "def load_csv_with_auto_sep(path):\n",    "    encoding = detect_encoding(path)\n",    "    if encoding is None:\n",    "        encoding = 'windows-1251'\n",    "    \n",    "    print(f\"   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∫–æ–¥—É–≤–∞–Ω–Ω—è: {encoding}\")\n",    "    \n",    "    try:\n",    "        with open(path, encoding=encoding) as f:\n",    "            sample = f.read(2048)\n",    "            dialect = csv.Sniffer().sniff(sample)\n",    "            sep = dialect.delimiter\n",    "    except:\n",    "        sep = ',' \n",    "    \n",    "    return pd.read_csv(\n",    "        path, \n",    "        encoding=encoding, \n",    "        sep=sep, \n",    "        on_bad_lines='skip',\n",    "        low_memory=False\n",    "    )\n",    "\n",    "def transform_columns(df, mapping):\n",    "    mapping_lower = {key.lower(): value for key, value in mapping.items()}\n",    "    \n",    "    new_columns = []\n",    "    for col in df.columns:\n",    "        col_lower = col.lower()\n",    "        if col_lower in mapping_lower:\n",    "            new_columns.append(mapping_lower[col_lower])\n",    "        else:\n",    "            new_columns.append(col)  \n",    "    df.columns = new_columns\n",    "    return df\n",    "\n",    "for filename in os.listdir(input_folder):\n",    "    if filename.endswith(\".csv\"):\n",    "        input_path = os.path.join(input_folder, filename)\n",    "        output_path = os.path.join(output_folder, filename)\n",    "        print(f\"üîÑ –û–±—Ä–æ–±–∫–∞ {filename}...\")\n",    "        \n",    "        try:\n",    "            df = load_csv_with_auto_sep(input_path)\n",    "            print(f\"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} —Ä—è–¥–∫—ñ–≤, {len(df.columns)} —Å—Ç–æ–≤–ø—Ü—ñ–≤\")\n",    "            \n",    "            dataset_type, mapping = detect_mapping(df)\n",    "\n",    "            if mapping is None:\n",    "                print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {filename}: –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ–≥–æ –º–∞–ø–ø—ñ–Ω–≥—É –∫–æ–ª–æ–Ω–æ–∫\")\n",    "                print(f\"   –î–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns[:10])}...\") \n",    "                continue\n",    "\n",    "            print(f\"   –í–∏–∑–Ω–∞—á–µ–Ω–æ —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç—É: {dataset_type}\")\n",    "                                    \n",    "            df = transform_columns(df, mapping)\n",    "            \n",    "            df.to_csv(output_path, index=False, encoding='utf-8')\n",    "            print(f\"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π —Ñ–∞–π–ª –¥–æ {output_path}\")\n",    "            print(f\"   –ù–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns[:10])}...\") \n",    "\n",    "        except Exception as e:\n",    "            print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ —É —Ñ–∞–π–ª—ñ {filename}: {e}\")\n",    "            import traceback\n",    "            traceback.print_exc()\n",    "\n",    "print(f\"\\nüéâ –û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–±—Ä–æ–±–ª–µ–Ω—ñ —Ñ–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É –ø–∞–ø—Ü—ñ '{output_folder}'\")"   ],   "id": "f7a2e52f6fceb2fd",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "This script is designed to automatically process CSV files located in the `data` folder. Its purpose is to unify the data structure: determine the type of dataset (e.g., ZNO or NMT), rename columns according to the specified mapping (stored in the `config` variable), and save the processed files to a new folder called `transformed_data`.\n",    "\n",    "During processing, the script automatically determines the encoding of each CSV file using the `chardet` library in order to read the data correctly. It then attempts to determine the separator (comma, semicolon, etc.) using `csv.Sniffer`. If this fails, a comma is used as the default option. After that, the data is loaded into `pandas.DataFrame`, while corrupted rows are ignored.\n",    "\n",    "After loading, the script compares the file column names with the mappings in `config` and tries to determine which dataset type the file corresponds to. If the column match exceeds 40%, the file is considered to belong to this type, and the columns are renamed according to the mapping. All processed files are saved in CSV format with UTF-8 encoding in the `transformed_data` folder.\n",    "\n",    "If the dataset type cannot be determined or another error occurs during processing, the script simply skips the file and moves on to the next one, leaving a message about the problem in the console. Thus, the script allows you to quickly process a large number of files with different structures and bring them to a single standard.\n"   ],   "id": "925def0c80f2e70e"  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Adding some statistic columns (or flags)",   "id": "5fb57a45bf6e9e1a"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import pandas as pd\n",    "import os\n",    "import csv\n",    "import chardet\n",    "import numpy as np\n",    "\n",    "input_folder = \"transformed_data\"\n",    "\n",    "os.makedirs(output_folder, exist_ok=True)\n",    "\n",    "EASTERN_REGIONS = ['–î–æ–Ω–µ—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–õ—É–≥–∞–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–•–∞—Ä–∫—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–î–Ω—ñ–ø—Ä–æ–ø–µ—Ç—Ä–æ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å']\n",    "WESTERN_REGIONS = ['–õ—å–≤—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–Ü–≤–∞–Ω–æ-–§—Ä–∞–Ω–∫—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–¢–µ—Ä–Ω–æ–ø—ñ–ª—å—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ó–∞–∫–∞—Ä–ø–∞—Ç—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ß–µ—Ä–Ω—ñ–≤–µ—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å']\n",    "NORTHERN_REGIONS = ['–í–æ–ª–∏–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–†—ñ–≤–Ω–µ–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–°—É–º—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ß–µ—Ä–Ω—ñ–≥—ñ–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ñ–∏—Ç–æ–º–∏—Ä—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å']\n",    "SOUTHERN_REGIONS = ['–û–¥–µ—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ú–∏–∫–æ–ª–∞—ó–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–•–µ—Ä—Å–æ–Ω—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ó–∞–ø–æ—Ä—ñ–∑—å–∫–∞ –æ–±–ª–∞—Å—Ç—å']\n",    "CENTRAL_REGIONS = ['–ö–∏—ó–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ß–µ—Ä–∫–∞—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ü–æ–ª—Ç–∞–≤—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–ö—ñ—Ä–æ–≤–æ–≥—Ä–∞–¥—Å—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–í—ñ–Ω–Ω–∏—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å', '–•–º–µ–ª—å–Ω–∏—Ü—å–∫–∞ –æ–±–ª–∞—Å—Ç—å']\n",    "\n",    "def detect_encoding(file_path):\n",    "    with open(file_path, 'rb') as f:\n",    "        sample = f.read(10000)\n",    "        result = chardet.detect(sample)\n",    "        return result['encoding']\n",    "\n",    "def load_csv_with_auto_sep(path):\n",    "    encoding = detect_encoding(path)\n",    "    if encoding is None:\n",    "        encoding = 'utf-8'\n",    "    print(f\"   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∫–æ–¥—É–≤–∞–Ω–Ω—è: {encoding}\")\n",    "    \n",    "    try:\n",    "        with open(path, encoding=encoding) as f:\n",    "            sample = f.read(2048)\n",    "            dialect = csv.Sniffer().sniff(sample)\n",    "            sep = dialect.delimiter\n",    "    except:\n",    "        sep = ','\n",    "\n",    "    return pd.read_csv(\n",    "        path,\n",    "        encoding=encoding,\n",    "        sep=sep,\n",    "        on_bad_lines='skip',\n",    "        low_memory=False\n",    "    )\n",    "\n",    "def detect_dataset_type_by_filename(filename):\n",    "    import re\n",    "    match = re.search(r'(\\d{4})', filename)\n",    "    if match:\n",    "        year = int(match.group(1))\n",    "        if year < 2022:\n",    "            return 'ZNO'\n",    "        else:\n",    "            return 'NMT'\n",    "    return None\n",    "\n",    "def add_flags_and_features(df, dataset_type):\n",    "    import re\n",    "    score_cols = [col for col in df.columns if 'score_100' in col]\n",    "    for col in score_cols:\n",    "        df[col] = df[col].astype(str).str.replace(',', '.', regex=False)\n",    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",    "\n",    "    print(f\"   –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è {len(score_cols)} score_100 —Å—Ç–æ–≤–ø—Ü—ñ–≤:\")\n",    "    for col in score_cols:\n",    "        non_null_count = df[col].notna().sum()\n",    "        positive_count = (df[col] > 0).sum() if non_null_count > 0 else 0\n",    "        print(f\"     {col}: {non_null_count} –Ω–µ-NaN, {positive_count} > 0\")\n",    "                                \n",    "    if dataset_type == 'NMT' and 'test_date' in df.columns:\n",    "        df['test_date'] = pd.to_datetime(df['test_date'], format='%d.%m.%Y', errors='coerce')\n",    "    \n",    "    if 'birth_year' in df.columns:\n",    "        df['birth_year'] = pd.to_numeric(df['birth_year'], errors='coerce')\n",    "        if dataset_type == 'ZNO':\n",    "            match = re.search(r'(\\d{4})', filename)\n",    "            if match:\n",    "                test_year = int(match.group(1))\n",    "                df['student_age'] = test_year - df['birth_year']\n",    "        elif dataset_type == 'NMT':\n",    "            df['student_age'] = df['test_date'].dt.year - df['birth_year']\n",    "        df['student_age'] = df['student_age'].clip(lower=0).astype('Int64')\n",    "    else:\n",    "        print(f\"   –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: —Å—Ç–æ–≤–ø–µ—Ü—å 'birth_year' –≤—ñ–¥—Å—É—Ç–Ω—ñ–π, 'student_age' –Ω–µ —Ä–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω–æ\")\n",    "        \n",    "    def classify_region(region):\n",    "        if pd.isna(region):\n",    "            return 'other'\n",    "        region = region.strip()\n",    "        if region in EASTERN_REGIONS:\n",    "            return 'east'\n",    "        elif region in WESTERN_REGIONS:\n",    "            return 'west'\n",    "        elif region in NORTHERN_REGIONS:\n",    "            return 'north'\n",    "        elif region in SOUTHERN_REGIONS:\n",    "            return 'south'\n",    "        elif region in CENTRAL_REGIONS:\n",    "            return 'central'\n",    "        else:\n",    "            return 'other'\n",    "\n",    "    df['region_flag'] = df['region_name'].apply(classify_region)\n",    "\n",    "    subject_cols = {\n",    "        'NMT': {\n",    "            'ukrainian': 'ukrainian_score_100',\n",    "            'math': 'math_score_100',\n",    "            'history': 'history_score_100',\n",    "            'physics': 'physics_score_100',\n",    "            'chemistry': 'chemistry_score_100',\n",    "            'biology': 'biology_score_100',\n",    "            'geography': 'geography_score_100',\n",    "            'english': 'english_score_100',\n",    "            'french': 'french_score_100',\n",    "            'german': 'german_score_100',\n",    "            'spanish': 'spanish_score_100',\n",    "            'ukrainian_literature': 'ukrainian_lit_score_100'\n",    "        },\n",    "        'ZNO': {\n",    "            'ukrainian': 'ukrainian_score_100',\n",    "            'math': 'math_score_100',\n",    "            'history': 'history_score_100',\n",    "            'physics': 'physics_score_100',\n",    "            'chemistry': 'chemistry_score_100',\n",    "            'biology': 'biology_score_100',\n",    "            'geography': 'geography_score_100',\n",    "            'english': 'english_score_100',\n",    "            'french': 'french_score_100',\n",    "            'german': 'german_score_100',\n",    "            'spanish': 'spanish_score_100'\n",    "        }\n",    "    }\n",    "\n",    "    subjects_taken = pd.Series(0, index=df.index, dtype='Int64')\n",    "    for subject, col in subject_cols[dataset_type].items():\n",    "        if col in df.columns:\n",    "            subjects_taken += (df[col].notna() & (df[col] >= 0)).astype('Int64')\n",    "    df['subjects_count'] = subjects_taken\n",    "\n",    "    score_cols = [col for col in df.columns if 'score_100' in col]\n",    "    df['total_score'] = df[score_cols].sum(axis=1, skipna=True)\n",    "    df['average_score'] = df[score_cols].mean(axis=1, skipna=True)\n",    "\n",    "    print(f\"   –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞: total_score –Ω–µ–Ω—É–ª—å–æ–≤–µ: {(df['total_score'] > 0).sum()}, \"f\"average_score non-NaN: {df['average_score'].notna().sum()}\")\n",    "    # if 'education_org_type' in df.columns:\n",    "    #     df['education_org_type'] = df['education_org_type'].str.strip().str.lower()\n",    "    #     df['education_org_type'] = df['education_org_type'].replace({\n",    "    #         '–ª—ñ—Ü–µ–π': '–ª—ñ—Ü–µ–π',\n",    "    #         '—à–∫–æ–ª–∞': '—à–∫–æ–ª–∞',\n",    "    #         '—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç': '—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç',\n",    "    #         '–∫–æ–ª–µ–¥–∂': '–∫–æ–ª–µ–¥–∂'\n",    "    #     })\n",    "\n",    "    return df\n",    "\n",    "for filename in os.listdir(input_folder):\n",    "    if filename.endswith(\".csv\"):\n",    "        input_path = os.path.join(input_folder, filename)\n",    "        print(f\"üîÑ –û–±—Ä–æ–±–∫–∞ {filename}...\")\n",    "\n",    "        try:\n",    "            df = load_csv_with_auto_sep(input_path)\n",    "            print(f\"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} —Ä—è–¥–∫—ñ–≤, {len(df.columns)} —Å—Ç–æ–≤–ø—Ü—ñ–≤\")\n",    "            \n",    "            dataset_type = detect_dataset_type_by_filename(filename)\n",    "            if dataset_type is None:\n",    "                print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ {filename}: –Ω–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç—É\")\n",    "                print(f\"   –î–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns[:10])}...\")\n",    "                continue\n",    "\n",    "            print(f\"   –í–∏–∑–Ω–∞—á–µ–Ω–æ —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç—É: {dataset_type}\")\n",    "                                    \n",    "            df = add_flags_and_features(df, dataset_type)\n",    "\n",    "            df.to_csv(input_path, index=False, encoding='utf-8')\n",    "            print(f\"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π —Ñ–∞–π–ª –¥–æ {input_path}\")\n",    "            print(f\"   –ù–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns[-10:])}...\") \n",    "\n",    "        except Exception as e:\n",    "            print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ —É —Ñ–∞–π–ª—ñ {filename}: {e}\")\n",    "            import traceback\n",    "            traceback.print_exc()\n",    "\n",    "print(f\"\\nüéâ –û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å—ñ –∑–º—ñ–Ω–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É –ø–∞–ø—Ü—ñ '{input_folder}'\")"   ],   "id": "21271297a1656521",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "This script is designed to add new attributes (flags) to CSV files that have already been transformed and are stored in the `transformed_data` folder. Its main purpose is to enrich the data: it calculates the age of students, the number of subjects taken, the average and total score, and classifies regions according to the macro-regional division of Ukraine (west, east, centre, south, north, other).\n",    "\n",    "The script works in several stages. First, it goes through each CSV file in the `transformed_data` folder, automatically determines the encoding and separator in the file, and loads it into `DataFrame`. Next, the type of dataset ‚Äî ZNO or NMT ‚Äî is determined based on the year in the file name: if the year is up to and including 2022, it is ZNO, otherwise it is NMT.\n",    "\n",    "Based on the dataset type, the `add_flags_and_features` function is launched, which performs several important transformations. All columns with scores (`score_100`) are first converted to numeric format. Next, if the columns `birth_year` and `test_date` (for NMT) are available, the student's age at the time of testing is calculated. If the column `region_name` is present, each row is assigned a geographical flag (e.g., east, west, etc.) according to the macro-region to which the area belongs.\n",    "\n",    "In addition, the number of subjects that the student has actually passed is counted (only non-null and non-negative values in the score columns are taken into account), and the total (`total_score`) and average (`average_score`) scores are calculated.\n",    "\n",    "After processing, all new columns are saved directly to the same file that was downloaded, with UTF-8 encoding. As a result, we obtain supplemented datasets with unified numerical values, regional references, and basic demographic information. If errors occur during processing, the script displays a corresponding message but does not interrupt the execution for other files.\n"   ],   "id": "918c7cf6fe4a3b3"  },  {   "metadata": {},   "cell_type": "markdown",   "source": [    "## Dropping columns\n",    "#### This script was written when there were problems with flag counting in the previous code, removing columns to reuse the previous script. "   ],   "id": "b86747c3695d0396"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import pandas as pd\n",    "import os\n",    "import csv\n",    "import chardet\n",    "\n",    "input_folder = \"transformed_data\"\n",    "os.makedirs(input_folder, exist_ok=True)\n",    "\n",    "columns_to_drop = [\n",    "    'total_score', 'average_score', 'subjects_count', 'region_flag',\n",    "]\n",    "\n",    "def detect_encoding(file_path):\n",    "    with open(file_path, 'rb') as f:\n",    "        sample = f.read(10000)\n",    "        result = chardet.detect(sample)\n",    "        return result['encoding']\n",    "\n",    "def load_csv_with_auto_sep(path):\n",    "    encoding = detect_encoding(path)\n",    "    if encoding is None:\n",    "        encoding = 'utf-8'\n",    "    print(f\"   –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∫–æ–¥—É–≤–∞–Ω–Ω—è: {encoding}\")\n",    "    \n",    "    try:\n",    "        with open(path, encoding=encoding) as f:\n",    "            sample = f.read(2048)\n",    "            dialect = csv.Sniffer().sniff(sample)\n",    "            sep = dialect.delimiter\n",    "    except:\n",    "        sep = ','\n",    "\n",    "    return pd.read_csv(\n",    "        path,\n",    "        encoding=encoding,\n",    "        sep=sep,\n",    "        on_bad_lines='skip',\n",    "        low_memory=False\n",    "    )\n",    "\n",    "for filename in os.listdir(input_folder):\n",    "    if filename.endswith(\".csv\"):\n",    "        input_path = os.path.join(input_folder, filename)\n",    "        print(f\"üîÑ –û–±—Ä–æ–±–∫–∞ {filename}...\")\n",    "\n",    "        try:\n",    "            df = load_csv_with_auto_sep(input_path)\n",    "            print(f\"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} —Ä—è–¥–∫—ñ–≤, {len(df.columns)} —Å—Ç–æ–≤–ø—Ü—ñ–≤\")\n",    "\n",    "            existing_columns = [col for col in columns_to_drop if col in df.columns]\n",    "            if not existing_columns:\n",    "                print(f\"   –ñ–æ–¥–Ω–∞ –∑ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ.\")\n",    "                continue\n",    "\n",    "            df = df.drop(columns=existing_columns, errors='ignore')\n",    "            print(f\"   –í–∏–¥–∞–ª–µ–Ω–æ {len(existing_columns)} –∫–æ–ª–æ–Ω–æ–∫: {existing_columns}\")\n",    "            print(f\"   –ó–∞–ª–∏—à–∏–ª–æ—Å—è {len(df.columns)} —Å—Ç–æ–≤–ø—Ü—ñ–≤\")\n",    "\n",    "            df.to_csv(input_path, index=False, encoding='utf-8')\n",    "            print(f\"‚úÖ –ó–±–µ—Ä–µ–∂–µ–Ω–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π —Ñ–∞–π–ª –¥–æ {input_path}\")\n",    "\n",    "        except Exception as e:\n",    "            print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ —É —Ñ–∞–π–ª—ñ {filename}: {e}\")\n",    "            import traceback\n",    "            traceback.print_exc()\n",    "\n",    "print(f\"\\nüéâ –û–±—Ä–æ–±–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–Ω–æ–≤–ª–µ–Ω—ñ —Ñ–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É –ø–∞–ø—Ü—ñ '{input_folder}'\")"   ],   "id": "1aa1a50b904282e",   "outputs": [],   "execution_count": null  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 2   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython2",   "version": "2.7.6"  } }, "nbformat": 4, "nbformat_minor": 5}