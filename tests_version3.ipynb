{ "cells": [  {   "metadata": {},   "cell_type": "markdown",   "source": [    "**–ù—É–ª—å–æ–≤–∞ –≥—ñ–ø–æ—Ç–µ–∑–∞ (H‚ÇÄ):**\n",    "–°–µ—Ä–µ–¥–Ω—ñ–π –±–∞–ª –∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ —É —á–æ–ª–æ–≤—ñ–∫—ñ–≤ —ñ –∂—ñ–Ω–æ–∫ –Ω–µ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ –∑–Ω–∞—á—É—â–æ.\n",    "\n",    "**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞ –≥—ñ–ø–æ—Ç–µ–∑–∞ (H‚ÇÅ):**\n",    "–°–µ—Ä–µ–¥–Ω—ñ–π –±–∞–ª –∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ —É —á–æ–ª–æ–≤—ñ–∫—ñ–≤ —ñ –∂—ñ–Ω–æ–∫ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–Ω–æ –∑–Ω–∞—á—É—â–æ."   ],   "id": "d833a390c38f5a10"  },  {   "cell_type": "code",   "id": "initial_id",   "metadata": {    "collapsed": true   },   "source": [    "def analyze_gender_math(df, dataset_type, filename, all_results):\n",    "    \"\"\"–ê–Ω–∞–ª—ñ–∑—É—î —Ä—ñ–∑–Ω–∏—Ü—é –≤ –æ—Ü—ñ–Ω–∫–∞—Ö –∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∑–∞ —Å—Ç–∞—Ç—Ç—é.\"\"\"\n",    "    required_cols = ['math_score_100', 'gender']\n",    "    if not all(col in df.columns for col in required_cols):\n",    "        print(f\"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {required_cols} —É {filename}\")\n",    "        return pd.Series(), pd.Series()\n",    "\n",    "    df_math = df[df['math_score_100'].notna() & df['gender'].isin(['—á–æ–ª–æ–≤—ñ—á–∞', '–∂—ñ–Ω–æ—á–∞'])]\n",    "    male_scores = df_math[df_math['gender'] == '—á–æ–ª–æ–≤—ñ—á–∞']['math_score_100']\n",    "    female_scores = df_math[df_math['gender'] == '–∂—ñ–Ω–æ—á–∞']['math_score_100']\n",    "\n",    "    result = {'file': filename, 'dataset_type': dataset_type}\n",    "    if len(male_scores) >= 30 and len(female_scores) >= 30:\n",    "        t_stat, p_value = ttest_ind(male_scores, female_scores, equal_var=False, nan_policy='omit')\n",    "        u_stat, u_pval = mannwhitneyu(male_scores, female_scores, alternative='two-sided')\n",    "        pooled_std = np.sqrt(((male_scores.std() ** 2) + (female_scores.std() ** 2)) / 2)\n",    "        cohen_d = (male_scores.mean() - female_scores.mean()) / pooled_std if pooled_std > 0 else np.nan\n",    "        result.update({\n",    "            'male_count': len(male_scores), 'female_count': len(female_scores),\n",    "            'male_avg': male_scores.mean(), 'female_avg': female_scores.mean(),\n",    "            't_stat': t_stat, 'p_value': p_value, 'significant': p_value < 0.05, 'u_stat': u_stat,\n",    "            'u_pval': u_pval, 'u_significant': u_pval < 0.05,'cohen_d': cohen_d\n",    "        })\n",    "    else:\n",    "        result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: —á–æ–ª–æ–≤—ñ–∫—ñ–≤={len(male_scores)}, –∂—ñ–Ω–æ–∫={len(female_scores)}'\n",    "\n",    "    all_results.append(result)\n",    "    return male_scores, female_scores"   ],   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def analyze_gender_ukrainian(df, dataset_type, filename, all_results):\n",    "    \"\"\"–ê–Ω–∞–ª—ñ–∑—É—î —Ä—ñ–∑–Ω–∏—Ü—é –≤ –æ—Ü—ñ–Ω–∫–∞—Ö –∑ —É–∫—Ä–∞—ó—Å—å–Ω–∫–æ—ó –∑–∞ —Å—Ç–∞—Ç—Ç—é.\"\"\"\n",    "    required_cols = ['ukrainian_score_100', 'gender']\n",    "    if not all(col in df.columns for col in required_cols):\n",    "        print(f\"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {required_cols} —É {filename}\")\n",    "        return pd.Series(), pd.Series()\n",    "    \n",    "    df_math = df[df['ukrainian_score_100'].notna() & df['gender'].isin(['—á–æ–ª–æ–≤—ñ—á–∞', '–∂—ñ–Ω–æ—á–∞'])]\n",    "    male_scores = df_math[df_math['gender'] == '—á–æ–ª–æ–≤—ñ—á–∞']['ukrainian_score_100']\n",    "    female_scores = df_math[df_math['gender'] == '–∂—ñ–Ω–æ—á–∞']['ukrainian_score_100']\n",    "    \n",    "    result = {'file': filename, 'dataset_type': dataset_type}\n",    "    if len(male_scores) >= 30 and len(female_scores) >= 30:\n",    "        t_stat, p_value = ttest_ind(male_scores, female_scores, equal_var=False, nan_policy='omit')\n",    "        u_stat, u_pval = mannwhitneyu(male_scores, female_scores, alternative='two-sided')\n",    "        pooled_std = np.sqrt(((male_scores.std() ** 2) + (female_scores.std() ** 2)) / 2)\n",    "        cohen_d = (male_scores.mean() - female_scores.mean()) / pooled_std if pooled_std > 0 else np.nan\n",    "        result.update({\n",    "            'male_count': len(male_scores), 'female_count': len(female_scores),\n",    "            'male_avg': male_scores.mean(), 'female_avg': female_scores.mean(),\n",    "            't_stat': t_stat, 'p_value': p_value, 'significant': p_value < 0.05, 'u_stat': u_stat,\n",    "            'u_pval': u_pval, 'u_significant': u_pval < 0.05, 'cohen_d': cohen_d\n",    "        })\n",    "    else:\n",    "        result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: —á–æ–ª–æ–≤—ñ–∫—ñ–≤={len(male_scores)}, –∂—ñ–Ω–æ–∫={len(female_scores)}'\n",    "    \n",    "    all_results.append(result)\n",    "    return male_scores, female_scores"   ],   "id": "ed9bca1241bfdc2",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def analyze_gender_history(df, dataset_type, filename, all_results):\n",    "    \"\"\"–ê–Ω–∞–ª—ñ–∑—É—î —Ä—ñ–∑–Ω–∏—Ü—é –≤ –æ—Ü—ñ–Ω–∫–∞—Ö –∑ —É–∫—Ä–∞—ó—Å—å–Ω–∫–æ—ó –∑–∞ —Å—Ç–∞—Ç—Ç—é.\"\"\"\n",    "    required_cols = ['history_score_100', 'gender']\n",    "    if not all(col in df.columns for col in required_cols):\n",    "        print(f\"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {required_cols} —É {filename}\")\n",    "        return pd.Series(), pd.Series()\n",    "    \n",    "    df_math = df[df['history_score_100'].notna() & df['gender'].isin(['—á–æ–ª–æ–≤—ñ—á–∞', '–∂—ñ–Ω–æ—á–∞'])]\n",    "    male_scores = df_math[df_math['gender'] == '—á–æ–ª–æ–≤—ñ—á–∞']['history_score_100']\n",    "    female_scores = df_math[df_math['gender'] == '–∂—ñ–Ω–æ—á–∞']['history_score_100']\n",    "    \n",    "    result = {'file': filename, 'dataset_type': dataset_type}\n",    "    if len(male_scores) >= 30 and len(female_scores) >= 30:\n",    "        t_stat, p_value = ttest_ind(male_scores, female_scores, equal_var=False, nan_policy='omit')\n",    "        u_stat, u_pval = mannwhitneyu(male_scores, female_scores, alternative='two-sided')\n",    "        pooled_std = np.sqrt(((male_scores.std() ** 2) + (female_scores.std() ** 2)) / 2)\n",    "        cohen_d = (male_scores.mean() - female_scores.mean()) / pooled_std if pooled_std > 0 else np.nan\n",    "        result.update({\n",    "            'male_count': len(male_scores), 'female_count': len(female_scores),\n",    "            'male_avg': male_scores.mean(), 'female_avg': female_scores.mean(),\n",    "            't_stat': t_stat, 'p_value': p_value, 'significant': p_value < 0.05, 'u_stat': u_stat,\n",    "            'u_pval': u_pval, 'u_significant': u_pval < 0.05, 'cohen_d': cohen_d\n",    "        })\n",    "    else:\n",    "        result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: —á–æ–ª–æ–≤—ñ–∫—ñ–≤={len(male_scores)}, –∂—ñ–Ω–æ–∫={len(female_scores)}'\n",    "    \n",    "    all_results.append(result)\n",    "    return male_scores, female_scores"   ],   "id": "dc63dc7dc1286f67",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def analyze_gender_english(df, dataset_type, filename, all_results):\n",    "    \"\"\"–ê–Ω–∞–ª—ñ–∑—É—î —Ä—ñ–∑–Ω–∏—Ü—é –≤ –æ—Ü—ñ–Ω–∫–∞—Ö –∑ —É–∫—Ä–∞—ó—Å—å–Ω–∫–æ—ó –∑–∞ —Å—Ç–∞—Ç—Ç—é.\"\"\"\n",    "    required_cols = ['english_score_100', 'gender']\n",    "    if not all(col in df.columns for col in required_cols):\n",    "        print(f\"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {required_cols} —É {filename}\")\n",    "        return pd.Series(), pd.Series()\n",    "    \n",    "    df_math = df[df['english_score_100'].notna() & df['gender'].isin(['—á–æ–ª–æ–≤—ñ—á–∞', '–∂—ñ–Ω–æ—á–∞'])]\n",    "    male_scores = df_math[df_math['gender'] == '—á–æ–ª–æ–≤—ñ—á–∞']['english_score_100']\n",    "    female_scores = df_math[df_math['gender'] == '–∂—ñ–Ω–æ—á–∞']['english_score_100']\n",    "    \n",    "    result = {'file': filename, 'dataset_type': dataset_type}\n",    "    if len(male_scores) >= 30 and len(female_scores) >= 30:\n",    "        t_stat, p_value = ttest_ind(male_scores, female_scores, equal_var=False, nan_policy='omit')\n",    "        u_stat, u_pval = mannwhitneyu(male_scores, female_scores, alternative='two-sided')\n",    "        pooled_std = np.sqrt(((male_scores.std() ** 2) + (female_scores.std() ** 2)) / 2)\n",    "        cohen_d = (male_scores.mean() - female_scores.mean()) / pooled_std if pooled_std > 0 else np.nan\n",    "        result.update({\n",    "            'male_count': len(male_scores), 'female_count': len(female_scores),\n",    "            'male_avg': male_scores.mean(), 'female_avg': female_scores.mean(),\n",    "            't_stat': t_stat, 'p_value': p_value, 'significant': p_value < 0.05, 'u_stat': u_stat,\n",    "            'u_pval': u_pval, 'u_significant': u_pval < 0.05, 'cohen_d': cohen_d\n",    "        })\n",    "    else:\n",    "        result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: —á–æ–ª–æ–≤—ñ–∫—ñ–≤={len(male_scores)}, –∂—ñ–Ω–æ–∫={len(female_scores)}'\n",    "    \n",    "    all_results.append(result)\n",    "    return male_scores, female_scores"   ],   "id": "9200f81604f722db",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import pandas as pd\n",    "import seaborn as sns\n",    "import matplotlib.pyplot as plt\n",    "\n",    "# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö\n",    "df_2020 = pd.read_csv('filtered_data/2020.csv')\n",    "df_2023 = pd.read_csv('filtered_data/2023.csv')\n",    "\n",    "# –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è: —Ç—ñ–ª—å–∫–∏ —Ç—ñ, —Ö—Ç–æ –º–∞—î –æ—Ü—ñ–Ω–∫—É –∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ —ñ –∑–∞–∑–Ω–∞—á–µ–Ω–∏–π –≥–µ–Ω–¥–µ—Ä\n",    "df_2020_math = df_2020[df_2020['math_score_100'].notna() & df_2020['gender'].isin(['—á–æ–ª–æ–≤—ñ—á–∞', '–∂—ñ–Ω–æ—á–∞'])].copy()\n",    "df_2023_math = df_2023[df_2023['math_score_100'].notna() & df_2023['gender'].isin(['—á–æ–ª–æ–≤—ñ—á–∞', '–∂—ñ–Ω–æ—á–∞'])].copy()\n",    "\n",    "# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤\n",    "plt.figure(figsize=(14, 6))\n",    "\n",    "# 2020\n",    "plt.subplot(1, 2, 1)\n",    "sns.histplot(\n",    "    data=df_2020_math,\n",    "    x='math_score_100',\n",    "    hue='gender',\n",    "    bins=50,\n",    "    element='step',\n",    "    stat='density',\n",    "    common_norm=False,\n",    "    palette='Set2'\n",    ")\n",    "plt.title('–†–æ–∑–ø–æ–¥—ñ–ª –±–∞–ª—ñ–≤ –∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ (2020)')\n",    "plt.xlabel('–ë–∞–ª–∏')\n",    "plt.ylabel('–©—ñ–ª—å–Ω—ñ—Å—Ç—å')\n",    "\n",    "# 2023\n",    "plt.subplot(1, 2, 2)\n",    "sns.histplot(\n",    "    data=df_2023_math,\n",    "    x='math_score_100',\n",    "    hue='gender',\n",    "    bins=50,\n",    "    element='step',\n",    "    stat='density',\n",    "    common_norm=False,\n",    "    palette='Set2'\n",    ")\n",    "plt.title('–†–æ–∑–ø–æ–¥—ñ–ª –±–∞–ª—ñ–≤ –∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ (2023)')\n",    "plt.xlabel('–ë–∞–ª–∏')\n",    "plt.ylabel('–©—ñ–ª—å–Ω—ñ—Å—Ç—å')\n",    "\n",    "# –í—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è\n",    "plt.tight_layout()\n",    "plt.show()\n"   ],   "id": "4ad9670f513f77bb",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "from scipy.stats import ttest_ind, mannwhitneyu\n",    "def analyze_abroad_results(df, dataset_type, filename, all_results):\n",    "    \"\"\"–ü–æ—Ä—ñ–≤–Ω—é—î –æ—Ü—ñ–Ω–∫–∏ –∑ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏ –∑–∞ –∫–æ—Ä–¥–æ–Ω–æ–º —ñ –≤ –£–∫—Ä–∞—ó–Ω—ñ (–ª–∏—à–µ –¥–ª—è –ù–ú–¢).\"\"\"\n",    "    if dataset_type != 'NMT':\n",    "        return pd.Series(), pd.Series()\n",    "    \n",    "    required_cols = ['math_score_100', 'region_flag']\n",    "    if not all(col in df.columns for col in required_cols):\n",    "        print(f\"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {required_cols} —É {filename}\")\n",    "        return pd.Series(), pd.Series()\n",    "    \n",    "    df_scores = df[df['average_score'].notna() & df['region_flag'].notna()]\n",    "    df_scores['location'] = np.where(df_scores['region_flag'] == 'other', '–ó–∞ –∫–æ—Ä–¥–æ–Ω–æ–º', '–£–∫—Ä–∞—ó–Ω–∞')\n",    "\n",    "    abroad_scores = df_scores[df_scores['location'] == '–ó–∞ –∫–æ—Ä–¥–æ–Ω–æ–º']['average_score']\n",    "    ukraine_scores = df_scores[df_scores['location'] == '–£–∫—Ä–∞—ó–Ω–∞']['average_score']\n",    "    \n",    "    result = {'file': filename, 'dataset_type': dataset_type}\n",    "    if len(abroad_scores) >= 30 and len(ukraine_scores) >= 30:\n",    "        t_stat, p_value = ttest_ind(abroad_scores, ukraine_scores, equal_var=False, nan_policy='omit')\n",    "        u_stat, u_pval = mannwhitneyu(abroad_scores, ukraine_scores, alternative='two-sided')\n",    "        pooled_std = np.sqrt(((abroad_scores.std() ** 2) + (ukraine_scores.std() ** 2)) / 2)\n",    "        cohen_d = (abroad_scores.mean() - ukraine_scores.mean()) / pooled_std if pooled_std > 0 else np.nan\n",    "        result.update({\n",    "            'abroad_count': len(abroad_scores), 'ukraine_count': len(ukraine_scores),\n",    "            'abroad_avg': abroad_scores.mean(), 'ukraine_avg': ukraine_scores.mean(),\n",    "            't_stat': t_stat, 'p_value': p_value, 'significant': p_value < 0.05, 'u_stat': u_stat,\n",    "            'u_pval': u_pval, 'u_significant': u_pval < 0.05, 'cohen_d': cohen_d\n",    "        })\n",    "    else:\n",    "        result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: –∑–∞ –∫–æ—Ä–¥–æ–Ω–æ–º={len(abroad_scores)}, –£–∫—Ä–∞—ó–Ω–∞={len(ukraine_scores)}'\n",    "    \n",    "    all_results.append(result)\n",    "    return abroad_scores, ukraine_scores"   ],   "id": "6d63d1bd06b88360",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# –ø–µ—Ä–µ–≤—ñ—Ä–∫–∞\n",    "import pandas as pd\n",    "df = pd.read_csv(\"filtered_data/2023.csv\", low_memory=False)\n",    "df_foreign = df[df['region_name'].str.contains(\"–Ü–Ω—à—ñ –∫—Ä–∞—ó–Ω–∏\", case=False, na=False)]\n",    "print(df_foreign)\n",    "df_foreign.to_csv(\"results/2023_foreign_testers.csv\", index=False, encoding='utf-8')"   ],   "id": "1e46dfbf65ea610f",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# from scipy.stats import spearmanr\n",    "# import pandas as pd\n",    "# import seaborn as sns\n",    "# import matplotlib.pyplot as plt\n",    "# import os\n",    "# \n",    "# def analyze_subject_correlation(df, dataset_type, filename, all_results):\n",    "#     \"\"\"–ê–Ω–∞–ª—ñ–∑—É—î –∫–æ—Ä–µ–ª—è—Ü—ñ—é –º—ñ–∂ –ø—Ä–µ–¥–º–µ—Ç–∞–º–∏, –æ–∫—Ä–µ–º–æ –¥–ª—è 2022 —ñ —ñ–Ω—à–∏—Ö —Ä–æ–∫—ñ–≤.\"\"\"\n",    "# \n",    "#     all_subjects = [\n",    "#         'ukrainian_score_100', 'math_score_100',\n",    "#         'biology_score_100', 'chemistry_score_100', 'physics_score_100',\n",    "#         'english_score_100', 'geography_score_100', 'ukrainian_literature_score_100',\n",    "#         'history_score_100', 'french_score_100', 'german_score_100', 'spanish_score_100'\n",    "#     ]\n",    "# \n",    "#     nmt_2022_subjects = ['ukrainian_score_100', 'math_score_100', 'history_score_100']\n",    "#     \n",    "#     if '2022' in filename and dataset_type == 'NMT':\n",    "#         subjects = nmt_2022_subjects\n",    "#     else:\n",    "#         subjects = all_subjects\n",    "#     \n",    "# \n",    "#     available_subjects = [s for s in subjects if s in df.columns]\n",    "#     print(f\"–î–æ—Å—Ç—É–ø–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ —É {filename}: {df.columns.tolist()}\")\n",    "#     print(f\"–ó–Ω–∞–π–¥–µ–Ω—ñ –ø—Ä–µ–¥–º–µ—Ç–∏: {available_subjects}\")\n",    "#     print(f\"–¢–∏–ø–∏ –¥–∞–Ω–∏—Ö: {df[available_subjects].dtypes}\")\n",    "#     \n",    "#     if len(available_subjects) < 2:\n",    "#         print(f\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø—Ä–µ–¥–º–µ—Ç—ñ–≤ –¥–ª—è –∫–æ—Ä–µ–ª—è—Ü—ñ—ó —É {filename}: {available_subjects}\")\n",    "#         result = {'file': filename, 'dataset_type': dataset_type, 'error': '–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –ø—Ä–µ–¥–º–µ—Ç—ñ–≤'}\n",    "#         all_results.append(result)\n",    "#         return pd.DataFrame()\n",    "#     \n",    "#     result = {'file': filename, 'dataset_type': dataset_type}\n",    "#     \n",    "#     corr_matrix = df[available_subjects].corr(method='spearman').round(2)\n",    "#     if len(df[available_subjects].dropna(how='all')) >= 30:\n",    "#         for i, s1 in enumerate(available_subjects):\n",    "#             for s2 in available_subjects[i+1:]:\n",    "#                 valid_data = df[[s1, s2]].dropna()\n",    "#                 if len(valid_data) >= 10:  \n",    "#                     corr, pval = spearmanr(valid_data[s1], valid_data[s2])\n",    "#                     result[f'{s1.replace(\"_score_100\", \"\")}_{s2.replace(\"_score_100\", \"\")}_corr'] = corr\n",    "#                     result[f'{s1.replace(\"_score_100\", \"\")}_{s2.replace(\"_score_100\", \"\")}_pval'] = pval\n",    "#                 else:\n",    "#                     result[f'{s1.replace(\"_score_100\", \"\")}_{s2.replace(\"_score_100\", \"\")}_corr'] = None\n",    "#                     result[f'{s1.replace(\"_score_100\", \"\")}_{s2.replace(\"_score_100\", \"\")}_pval'] = None\n",    "#         result['corr_matrix'] = corr_matrix\n",    "#         \n",    "#         # –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–µ–ø–ª–æ–≤–æ—ó –∫–∞—Ä—Ç–∏\n",    "#         plt.figure(figsize=(10, 8))\n",    "#         sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",    "#         plt.title(f'–ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è ({dataset_type}, {filename})')\n",    "#         plt.savefig(os.path.join('results/graphs', f'corr_matrix_{filename}.png'), dpi=300)\n",    "#         plt.close()\n",    "#     else:\n",    "#         result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: {len(df[available_subjects].dropna(how=\"all\"))} –∑–∞–ø–∏—Å—ñ–≤'\n",    "#     \n",    "#     all_results.append(result)\n",    "#     return df[available_subjects]"   ],   "id": "e74124edcc9ef533",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# def analyze_additional_subjects(df, dataset_type, filename, all_results):\n",    "#     \"\"\"–í–∏–∑–Ω–∞—á–∞—î —á–∞—Å—Ç–æ—Ç—É –≤–∏–±–æ—Ä—É –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –ø—Ä–µ–¥–º–µ—Ç—ñ–≤.\"\"\"\n",    "#     subjects = [\n",    "#         'biology_score_100', 'chemistry_score_100', 'physics_score_100',\n",    "#         'english_score_100', 'geography_score_100', 'ukrainian_literature_score_100',\n",    "#         'history_score_100', 'french_score_100', 'german_score_100', 'spanish_score_100',\n",    "#     ]\n",    "#     counts = {}\n",    "#     for subject in subjects:\n",    "#         if subject in df.columns:\n",    "#             counts[subject.replace('_score_100', '')] = df[subject].notna().sum()\n",    "#     \n",    "#     result = {'file': filename, 'dataset_type': dataset_type, **counts}\n",    "#     all_results.append(result)\n",    "#     return counts"   ],   "id": "ac139e29b8764ce0",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def analyze_age_groups(df, dataset_type, filename, all_results):\n",    "    \"\"\"–ü–æ—Ä—ñ–≤–Ω—é—î –æ—Ü—ñ–Ω–∫–∏ –∑ average_score –∑–∞ –≤—ñ–∫–æ–º (17/18).\"\"\"\n",    "    required_cols = ['average_score', 'student_age']\n",    "    if not all(col in df.columns for col in required_cols):\n",    "        print(f\"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {required_cols} —É {filename}\")\n",    "        return pd.Series(), pd.Series()\n",    "    \n",    "    df_age = df[df['average_score'].notna() & df['student_age'].isin([17, 18])]\n",    "    age_17_scores = df_age[df_age['student_age'] == 17]['average_score']\n",    "    age_18_scores = df_age[df_age['student_age'] == 18]['average_score']\n",    "    \n",    "    result = {'file': filename, 'dataset_type': dataset_type}\n",    "    if len(age_17_scores) >= 30 and len(age_18_scores) >= 30:\n",    "        t_stat, p_value = ttest_ind(age_17_scores, age_18_scores, equal_var=False, nan_policy='omit')\n",    "        u_stat, u_pval = mannwhitneyu(age_17_scores, age_18_scores, alternative='two-sided')\n",    "        pooled_std = np.sqrt(((age_17_scores.std() ** 2) + (age_18_scores.std() ** 2)) / 2)\n",    "        cohen_d = (age_17_scores.mean() - age_18_scores.mean()) / pooled_std if pooled_std > 0 else np.nan\n",    "        result.update({\n",    "            'age_17_count': len(age_17_scores), 'age_18_count': len(age_18_scores),\n",    "            'age_17_avg': age_17_scores.mean(), 'age_18_avg': age_18_scores.mean(),\n",    "            't_stat': t_stat, 'p_value': p_value, 'significant': p_value < 0.05, 'u_stat': u_stat,\n",    "            'u_pval': u_pval, 'u_significant': u_pval < 0.05, 'cohen_d': cohen_d\n",    "        })\n",    "    else:\n",    "        result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: 17={len(age_17_scores)}, 18={len(age_18_scores)}'\n",    "    \n",    "    all_results.append(result)\n",    "    return age_17_scores, age_18_scores"   ],   "id": "beeaac2b5d68e837",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# def analyze_age_group(df, dataset_type, filename, all_results):\n",    "#     \"\"\"–ü–æ—Ä—ñ–≤–Ω—é—î –æ—Ü—ñ–Ω–∫–∏ –∑ average_score –∑–∞ –≤—ñ–∫–æ–º (17/18).\"\"\"\n",    "#     required_cols = ['average_score', 'student_age']\n",    "#     if not all(col in df.columns for col in required_cols):\n",    "#         print(f\"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {required_cols} —É {filename}\")\n",    "#         return pd.Series(), pd.Series()\n",    "#     \n",    "#     df_age = df[df['average_score'].notna() & df['student_age'].isin([16, 19])]\n",    "#     age_16_scores = df_age[df_age['student_age'] == 16]['average_score']\n",    "#     age_17_scores = df_age[df_age['student_age'] == 19]['average_score']\n",    "#     \n",    "#     result = {'file': filename, 'dataset_type': dataset_type}\n",    "#     if len(age_16_scores) >= 30 and len(age_17_scores) >= 30:\n",    "#         t_stat, p_value = ttest_ind(age_16_scores, age_17_scores, equal_var=False, nan_policy='omit')\n",    "#         u_stat, u_pval = mannwhitneyu(age_16_scores, age_17_scores, alternative='two-sided')\n",    "#         pooled_std = np.sqrt(((age_16_scores.std() ** 2) + (age_17_scores.std() ** 2)) / 2)\n",    "#         cohen_d = (age_16_scores.mean() - age_17_scores.mean()) / pooled_std if pooled_std > 0 else np.nan\n",    "#         result.update({\n",    "#             'age_17_count': len(age_16_scores), 'age_18_count': len(age_17_scores),\n",    "#             'age_17_avg': age_16_scores.mean(), 'age_18_avg': age_17_scores.mean(),\n",    "#             't_stat': t_stat, 'p_value': p_value, 'significant': p_value < 0.05, 'u_stat': u_stat,\n",    "#             'u_pval': u_pval, 'u_significant': u_pval < 0.05, 'cohen_d': cohen_d\n",    "#         })\n",    "#     else:\n",    "#         result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: 17={len(age_16_scores)}, 18={len(age_17_scores)}'\n",    "#     \n",    "#     all_results.append(result)\n",    "#     return age_16_scores, age_17_scores"   ],   "id": "1731e635801443ae",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import pandas as pd\n",    "import numpy as np\n",    "from scipy.stats import kruskal\n",    "from scipy.stats import rankdata\n",    "\n",    "def analyze_education_org_type(df, dataset_type, filename, all_results):\n",    "    \"\"\"\n",    "    –ê–Ω–∞–ª—ñ–∑—É—î —Ç–∏–ø–∏ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –∑–∞–∫–ª–∞–¥—ñ–≤, –≥—Ä—É–ø—É—î —Ç–æ–ø-6 (2019-2021) –∞–±–æ —Ç–æ–ø-5 (2022-2024) —Ç–∏–ø—ñ–≤,\n",    "    —ñ –≤–∏–∫–æ–Ω—É—î –Ω–µ–ø–∞—Ä–∞–º–µ—Ç—Ä–∏—á–Ω–∏–π —Ç–µ—Å—Ç –ö—Ä—É—Å–∫–∞–ª–∞-–í–∞–ª–ª—ñ—Å–∞ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—ñ—Ö –æ—Ü—ñ–Ω–æ–∫.\n",    "    \"\"\"\n",    "    # –í–∏—Ç—è–≥—É—î–º–æ —Ä—ñ–∫ —ñ–∑ –Ω–∞–∑–≤–∏ —Ñ–∞–π–ª—É –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ç–æ–ø—É\n",    "    year = int(filename.split('.')[0])\n",    "\n",    "    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫\n",    "    required_cols = ['education_org_type', 'average_score']\n",    "    if not all(col in df.columns for col in required_cols):\n",    "        print(f\"‚ö†Ô∏è –í—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏ {required_cols} —É {filename}\")\n",    "        return pd.Series(), pd.Series()\n",    "\n",    "    # –§—ñ–ª—å—Ç—Ä—É—î–º–æ —Ä—è–¥–∫–∏ –∑ –≤–∞–ª—ñ–¥–Ω–∏–º–∏ –¥–∞–Ω–∏–º–∏\n",    "    df_scores = df[df['average_score'].notna() & df['education_org_type'].notna()].copy()\n",    "\n",    "    if 2019 <= year <= 2021:\n",    "        top_n = 6\n",    "    else: \n",    "        top_n = 5\n",    "\n",    "    # –ì—Ä—É–ø—É—î–º–æ –∑–∞ education_org_type —ñ –æ–±—á–∏—Å–ª—é—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",    "    type_summary = df_scores.groupby('education_org_type')['average_score'].agg(['count', 'mean']).reset_index()\n",    "    top_types = type_summary.sort_values('count', ascending=False).head(top_n)['education_org_type'].tolist()\n",    "\n",    "    # –î–æ–¥–∞—î–º–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—é \"—ñ–Ω—à—ñ\"\n",    "    df_scores['education_org_type_grouped'] = df_scores['education_org_type'].apply(\n",    "        lambda x: x if x in top_types else '—ñ–Ω—à—ñ'\n",    "    )\n",    "\n",    "    # –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –≥—Ä—É–ø –¥–ª—è —Ç–µ—Å—Ç—É\n",    "    groups = {}\n",    "    for name, group in df_scores.groupby('education_org_type_grouped'):\n",    "        groups[name] = group['average_score'].dropna().values\n",    "    group_names = list(groups.keys())\n",    "    group_data = [groups[name] for name in group_names]\n",    "\n",    "    # –†–µ–∑—É–ª—å—Ç–∞—Ç–∏\n",    "    result = {'file': filename, 'dataset_type': dataset_type}\n",    "\n",    "    if all(len(data) >= 30 for data in group_data):  # –ú—ñ–Ω—ñ–º—É–º 30 –∑–∞–ø–∏—Å—ñ–≤ –Ω–∞ –≥—Ä—É–ø—É\n",    "        # –¢–µ—Å—Ç –ö—Ä—É—Å–∫–∞–ª–∞-–í–∞–ª–ª—ñ—Å–∞\n",    "        stat, p_value = kruskal(*group_data)\n",    "        result.update({\n",    "            'stat': stat, 'p_value': p_value, 'significant': p_value < 0.05\n",    "        })\n",    "\n",    "        # –ï—Ñ–µ–∫—Ç —Ä–æ–∑–º—ñ—Ä—É (–∞–Ω–∞–ª–æ–≥ Cohen's d –¥–ª—è —Ä–∞–Ω–≥—ñ–≤, —Å–ø—Ä–æ—â–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è)\n",    "        all_ranks = rankdata(np.concatenate(group_data))\n",    "        group_ranks = [rankdata(data) for data in group_data]\n",    "        n_total = len(all_ranks)\n",    "        rank_sums = [sum(ranks) for ranks in group_ranks]\n",    "        n_groups = len(group_data)\n",    "        h_adjusted = stat * (n_total - 1) / (n_total - n_groups)\n",    "        epsilon_squared = h_adjusted / (n_total * (n_groups - 1))  # –ï—Ñ–µ–∫—Ç —Ä–æ–∑–º—ñ—Ä—É\n",    "        result['epsilon_squared'] = epsilon_squared\n",    "\n",    "        # –í–∏–∑–Ω–∞—á–∞—î–º–æ \"–∫—Ä–∞—â–∏–π\" —Ç–∏–ø (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞ —Å–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞ —Å–µ—Ä–µ–¥ —Ç–æ–ø-–≥—Ä—É–ø)\n",    "        top_group_means = {name: np.mean(data) for name, data in groups.items() if name != '—ñ–Ω—à—ñ'}\n",    "        if top_group_means:\n",    "            best_type = max(top_group_means, key=top_group_means.get)\n",    "            best_mean = top_group_means[best_type]\n",    "            result.update({\n",    "                'best_type': best_type,\n",    "                'best_mean': best_mean\n",    "            })\n",    "            print(f\"üîπ –ù–∞–π–∫—Ä–∞—â–∏–π —Ç–∏–ø –Ω–∞–≤—á–∞–ª—å–Ω–æ–≥–æ –∑–∞–∫–ª–∞–¥—É: {best_type} (—Å–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞: {best_mean:.2f})\")\n",    "        else:\n",    "            print(\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞–π–∫—Ä–∞—â–æ–≥–æ —Ç–∏–ø—É.\")\n",    "\n",    "    else:\n",    "        result['error'] = f'–ù–µ–¥–æ—Å—Ç–∞—Ç–Ω—å–æ –¥–∞–Ω–∏—Ö: {[len(data) for data in group_data]} –∑–∞–ø–∏—Å—ñ–≤ —É –≥—Ä—É–ø–∞—Ö'\n",    "\n",    "    all_results.append(result)\n",    "    return df_scores['education_org_type_grouped'], df_scores['average_score']"   ],   "id": "80a399b71781bff2",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import os\n",    "import pandas as pd\n",    "import numpy as np\n",    "from scipy.stats import ttest_ind, mannwhitneyu\n",    "import chardet\n",    "import re\n",    "import csv\n",    "import matplotlib.pyplot as plt\n",    "import seaborn as sns\n",    "from pathlib import Path\n",    "\n",    "# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ø–∞–ø–æ–∫\n",    "INPUT_FOLDER = \"filtered_data\"\n",    "RESULTS_FOLDER = \"results\"\n",    "GRAPHS_FOLDER = os.path.join(RESULTS_FOLDER, \"graphs\")\n",    "for folder in [RESULTS_FOLDER, GRAPHS_FOLDER]:\n",    "    os.makedirs(folder, exist_ok=True)\n",    "\n",    "sns.set_style(\"whitegrid\")\n",    "\n",    "def detect_encoding(file_path):\n",    "    \"\"\"–í–∏–∑–Ω–∞—á–∞—î –∫–æ–¥—É–≤–∞–Ω–Ω—è —Ñ–∞–π–ª—É.\"\"\"\n",    "    try:\n",    "        with open(file_path, 'rb') as f:\n",    "            return chardet.detect(f.read(10000))['encoding'] or 'utf-8'\n",    "    except Exception as e:\n",    "        print(f\"–ü–æ–º–∏–ª–∫–∞ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–æ–¥—É–≤–∞–Ω–Ω—è –¥–ª—è {file_path}: {e}\")\n",    "        return 'utf-8'\n",    "\n",    "def detect_separator(file_path, encoding):\n",    "    \"\"\"–í–∏–∑–Ω–∞—á–∞—î —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫ —É CSV-—Ñ–∞–π–ª—ñ.\"\"\"\n",    "    try:\n",    "        with open(file_path, encoding=encoding) as f:\n",    "            sample = f.read(2048)\n",    "            return csv.Sniffer().sniff(sample).delimiter\n",    "    except Exception as e:\n",    "        print(f\"–ü–æ–º–∏–ª–∫–∞ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫–∞ –¥–ª—è {file_path}: {e}\")\n",    "        return ','\n",    "\n",    "def load_csv(file_path):\n",    "    \"\"\"–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î CSV-—Ñ–∞–π–ª.\"\"\"\n",    "    encoding = detect_encoding(file_path)\n",    "    sep = detect_separator(file_path, encoding)\n",    "    print(f\"   –ö–æ–¥—É–≤–∞–Ω–Ω—è: {encoding}, —Ä–æ–∑–¥—ñ–ª—å–Ω–∏–∫: '{sep}'\")\n",    "    \n",    "    try:\n",    "        df = pd.read_csv(file_path, encoding=encoding, sep=sep, quotechar='\"', on_bad_lines='skip')\n",    "        print(f\"   –ü–µ—Ä—à—ñ 5 –∫–æ–ª–æ–Ω–æ–∫: {list(df.columns[:5])}\")\n",    "        return df\n",    "    except Exception as e:\n",    "        print(f\"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è {file_path}: {e}\")\n",    "        return None\n",    "\n",    "def detect_dataset_type(filename):\n",    "    \"\"\"–í–∏–∑–Ω–∞—á–∞—î —Ç–∏–ø –¥–∞—Ç–∞—Å–µ—Ç—É –∑–∞ –Ω–∞–∑–≤–æ—é —Ñ–∞–π–ª—É.\"\"\"\n",    "    match = re.search(r'(\\d{4})', filename)\n",    "    if match:\n",    "        year = int(match.group(1))\n",    "        return 'NMT' if year >= 2022 else 'ZNO'\n",    "    return None\n",    "\n",    "# –ó–±—ñ—Ä –¥–∞–Ω–∏—Ö\n",    "all_gender_results = []\n",    "all_gender_results2 = []\n",    "all_gender_results3 = []\n",    "all_gender_results4 = []\n",    "all_abroad_results = []\n",    "all_correlation_results = []\n",    "all_subjects_results = []\n",    "all_age_results = []\n",    "\n",    "age_results = []\n",    "education_results = []\n",    "\n",    "for filename in os.listdir(INPUT_FOLDER):\n",    "    if not filename.endswith(\".csv\"):\n",    "        continue\n",    "    \n",    "    path = os.path.join(INPUT_FOLDER, filename)\n",    "    print(f\"üîÑ –û–±—Ä–æ–±–∫–∞ {filename}...\")\n",    "    \n",    "    try:\n",    "        df = load_csv(path)\n",    "        if df is None or df.empty:\n",    "            print(f\"‚ö†Ô∏è –§–∞–π–ª {filename} –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –Ω–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏\")\n",    "            continue\n",    "        \n",    "        dataset_type = detect_dataset_type(filename)\n",    "        if not dataset_type:\n",    "            print(f\"‚ö†Ô∏è –ù–µ–≤—ñ–¥–æ–º–∏–π —Ç–∏–ø –¥–∞–Ω–∏—Ö: {filename}\")\n",    "            continue\n",    "        \n",    "        print(f\"   –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(df)} —Ä—è–¥–∫—ñ–≤, {len(df.columns)} —Å—Ç–æ–≤–ø—Ü—ñ–≤\")\n",    "        \n",    "        gender_male, gender_female = analyze_gender_math(df, dataset_type, filename, all_gender_results)\n",    "        gender_male2, gender_female2 = analyze_gender_ukrainian(df, dataset_type, filename, all_gender_results2)\n",    "        gender_male3, gender_female3 = analyze_gender_history(df, dataset_type, filename, all_gender_results3)\n",    "        gender_male4, gender_female4 = analyze_gender_english(df, dataset_type, filename, all_gender_results4)\n",    "        abroad_abroad, abroad_ukraine = analyze_abroad_results(df, dataset_type, filename, all_abroad_results)\n",    "        # subjects_df = analyze_subject_correlation(df, dataset_type, filename, all_correlation_results)\n",    "        # subjects_counts = analyze_additional_subjects(df, dataset_type, filename, all_subjects_results)\n",    "        age_17, age_18 = analyze_age_groups(df, dataset_type, filename, all_age_results)\n",    "        # age_16, age2_17 = analyze_age_group(df, dataset_type, filename, age_results)\n",    "        education_org_type = analyze_education_org_type(df, dataset_type, filename, education_results)\n",    "      \n",    "    except Exception as e:\n",    "        print(f\"‚ùå –ü–æ–º–∏–ª–∫–∞ —É —Ñ–∞–π–ª—ñ {filename}: {e}\")\n",    "        import traceback\n",    "        traceback.print_exc()\n",    "\n",    "# –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ —Ä–æ–∫–∞—Ö\n",    "pd.DataFrame(all_gender_results).to_csv(os.path.join(RESULTS_FOLDER, 'gender_math_analysis.csv'), index=False)\n",    "pd.DataFrame(all_gender_results2).to_csv(os.path.join(RESULTS_FOLDER, 'gender_ukrainian_analysis.csv'), index=False)\n",    "pd.DataFrame(all_gender_results3).to_csv(os.path.join(RESULTS_FOLDER, 'gender_history_analysis.csv'), index=False)\n",    "pd.DataFrame(all_gender_results4).to_csv(os.path.join(RESULTS_FOLDER, 'gender_english_analysis.csv'), index=False)\n",    "pd.DataFrame(all_abroad_results).to_csv(os.path.join(RESULTS_FOLDER, 'abroad_results_analysis.csv'), index=False)\n",    "# pd.DataFrame(all_correlation_results).to_csv(os.path.join(RESULTS_FOLDER, 'subject_correlation_analysis.csv'), index=False)\n",    "# pd.DataFrame(all_subjects_results).to_csv(os.path.join(RESULTS_FOLDER, 'additional_subjects_analysis.csv'), index=False)\n",    "pd.DataFrame(all_age_results).to_csv(os.path.join(RESULTS_FOLDER, 'age_groups_analysis.csv'), index=False)\n",    "pd.DataFrame(education_results).to_csv(os.path.join(RESULTS_FOLDER, 'education_type_analysis.csv'), index=False)\n",    "# pd.DataFrame(age_results).to_csv(os.path.join(RESULTS_FOLDER, 'age_group(16, 17)_analysis.csv'), index=False)\n",    "print(f\"\\nüéâ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ '{RESULTS_FOLDER}'\")"   ],   "id": "c5a6be50dbd063b8",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "print(\"\\nüîÑ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤...\")\n",    "# –ü–∏—Ç–∞–Ω–Ω—è 1: –ì–µ–Ω–¥–µ—Ä (–ø–æ —Ä–æ–∫–∞—Ö)\n",    "for result in all_gender_results:\n",    "    if 'male_avg' in result and 'female_avg' in result:\n",    "        plt.figure(figsize=(8, 6))\n",    "        plt.bar(['–ß–æ–ª–æ–≤—ñ–∫–∏', '–ñ—ñ–Ω–∫–∏'], [result['male_avg'], result['female_avg']],\n",    "                color=['#36A2EB', '#FF6384'], edgecolor=['#2E8B57', '#C71585'])\n",    "        plt.title(f'–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∑–∞ —Å—Ç–∞—Ç—Ç—é ({result[\"dataset_type\"]}, {result[\"file\"]})')\n",    "        plt.ylabel('–°–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞ (0-200)')\n",    "        plt.savefig(os.path.join(GRAPHS_FOLDER, f'gender_math_{result[\"file\"]}.png'), dpi=300, bbox_inches='tight')\n",    "        plt.close()\n",    "\n",    "# –ü–∏—Ç–∞–Ω–Ω—è 2: –ó–∞ –∫–æ—Ä–¥–æ–Ω–æ–º (–ø–æ —Ä–æ–∫–∞—Ö, –ª–∏—à–µ –ù–ú–¢)\n",    "for result in all_abroad_results:\n",    "    if 'abroad_avg' in result and 'ukraine_avg' in result:\n",    "        plt.figure(figsize=(8, 6))\n",    "        plt.bar(['–ó–∞ –∫–æ—Ä–¥–æ–Ω–æ–º', '–£–∫—Ä–∞—ó–Ω–∞'], [result['abroad_avg'], result['ukraine_avg']],\n",    "                color=['#FFCE56', '#4BC0C0'], edgecolor=['#FFD700', '#20B2AA'])\n",    "        plt.title(f'–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∑–∞ –º—ñ—Å—Ü–µ–º —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è (–ù–ú–¢, {result[\"file\"]})')\n",    "        plt.ylabel('–°–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞ (0-200)')\n",    "        plt.savefig(os.path.join(GRAPHS_FOLDER, f'abroad_results_{result[\"file\"]}.png'), dpi=300, bbox_inches='tight')\n",    "        plt.close()\n",    "\n",    "\n",    "# –ü–∏—Ç–∞–Ω–Ω—è 3: –ö–æ—Ä–µ–ª—è—Ü—ñ—è (–ø–æ —Ä–æ–∫–∞—Ö)\n",    "for result in all_correlation_results:\n",    "    if 'corr_matrix' in result:\n",    "        corr_matrix = result['corr_matrix']\n",    "        subjects = [s.replace('_score_100', '') for s in corr_matrix.columns]\n",    "        plt.figure(figsize=(10, 8))\n",    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",    "                    xticklabels=subjects, yticklabels=subjects)\n",    "        plt.title(f'–ö–æ—Ä–µ–ª—è—Ü—ñ—è –ø—Ä–µ–¥–º–µ—Ç—ñ–≤ ({result[\"dataset_type\"]}, {result[\"file\"]})')\n",    "        plt.savefig(os.path.join(GRAPHS_FOLDER, f'subject_correlation_{result[\"file\"]}.png'), dpi=300, bbox_inches='tight')\n",    "        plt.close()\n",    "\n",    "# –ü–∏—Ç–∞–Ω–Ω—è 4: –î–æ–¥–∞—Ç–∫–æ–≤—ñ –ø—Ä–µ–¥–º–µ—Ç–∏ (–ø–æ —Ä–æ–∫–∞—Ö)\n",    "for result in all_subjects_results:\n",    "    counts = {k: v for k, v in result.items() if k not in ['file', 'dataset_type'] and isinstance(v, (int, float)) and v > 0}\n",    "    if counts:\n",    "        plt.figure(figsize=(8, 6))\n",    "        plt.pie(list(counts.values()), labels=list(counts.keys()), colors=sns.color_palette('pastel'),\n",    "                autopct='%1.1f%%')\n",    "        plt.title(f'–ü–æ–ø—É–ª—è—Ä–Ω—ñ—Å—Ç—å –ø—Ä–µ–¥–º–µ—Ç—ñ–≤ ({result[\"dataset_type\"]}, {result[\"file\"]})')\n",    "        plt.savefig(os.path.join(GRAPHS_FOLDER, f'additional_subjects_{result[\"file\"]}.png'), dpi=300, bbox_inches='tight')\n",    "        plt.close()\n",    "    else:\n",    "        print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –≥—Ä–∞—Ñ—ñ–∫ –¥–ª—è {result['file']}: –Ω–µ–º–∞—î –∫–æ—Ä–µ–∫—Ç–Ω–∏—Ö –¥–∞–Ω–∏—Ö –¥–ª—è –ø—Ä–µ–¥–º–µ—Ç—ñ–≤ ({counts})\")\n",    "\n",    "# –ü–∏—Ç–∞–Ω–Ω—è 5: –í—ñ–∫–æ–≤—ñ –≥—Ä—É–ø–∏ (–ø–æ —Ä–æ–∫–∞—Ö)\n",    "for result in all_age_results:\n",    "    if 'age_17_avg' in result and 'age_18_avg' in result:\n",    "        plt.figure(figsize=(8, 6))\n",    "        plt.bar(['17 —Ä–æ–∫—ñ–≤', '18 —Ä–æ–∫—ñ–≤'], [result['age_17_avg'], result['age_18_avg']],\n",    "                color=['#FF6384', '#36A2EB'], edgecolor=['#DC143C', '#1E90FF'])\n",    "        plt.title(f'–ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞ –∑–∞ –≤—ñ–∫–æ–º ({result[\"dataset_type\"]}, {result[\"file\"]})')\n",    "        plt.ylabel('–°–µ—Ä–µ–¥–Ω—è –æ—Ü—ñ–Ω–∫–∞ (0-200)')\n",    "        plt.savefig(os.path.join(GRAPHS_FOLDER, f'age_groups_{result[\"file\"]}.png'), dpi=300, bbox_inches='tight')\n",    "        plt.close()\n",    "\n",    "print(f\"\\nüéâ –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ì—Ä–∞—Ñ—ñ–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤ '{GRAPHS_FOLDER}'\")"   ],   "id": "edaa416af057e110",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import pandas as pd\n",    "import numpy as np\n",    "import os\n",    "import seaborn as sns\n",    "import matplotlib.pyplot as plt\n",    "from sklearn.ensemble import RandomForestRegressor\n",    "from sklearn.preprocessing import OneHotEncoder\n",    "from sklearn.model_selection import train_test_split\n",    "\n",    "DATA_FOLDER = \"filtered_data\"\n",    "GRAPHS_FOLDER = \"results/graphs\"\n",    "os.makedirs(GRAPHS_FOLDER, exist_ok=True)\n",    "\n",    "csv_files = [f for f in os.listdir(DATA_FOLDER) if f.endswith(\".csv\")]\n",    "\n",    "required_features = ['territory_type', 'region_name', 'education_org_type', 'gender', 'student_age', 'subjects_count']\n",    "\n",    "all_importances = []\n",    "\n",    "for file in csv_files:\n",    "    path = os.path.join(DATA_FOLDER, file)\n",    "    print(f\"\\nüìÇ –û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É: {file}\")\n",    "\n",    "    df = pd.read_csv(path)\n",    "    \n",    "    if 'average_score' not in df.columns:\n",    "        print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª {file}, –±–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ–π —Å—Ç–æ–≤–ø—á–∏–∫ average_score\")\n",    "        continue\n",    "\n",    "    missing_features = [c for c in required_features if c not in df.columns]\n",    "    if missing_features:\n",    "        print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª {file}, –±–æ –≤—ñ–¥—Å—É—Ç–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏: {missing_features}\")\n",    "        continue\n",    "\n",    "    df = df.dropna(subset=['average_score'] + required_features)\n",    "\n",    "    if df.empty:\n",    "        print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª {file}, –±–æ –Ω–µ–º–∞—î –¥–∞–Ω–∏—Ö –ø—ñ—Å–ª—è –≤–∏–¥–∞–ª–µ–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å\")\n",    "        continue\n",    "\n",    "    y = df['average_score']\n",    "    X_raw = df[required_features]\n",    "\n",    "    categorical_columns = ['territory_type', 'region_name', 'education_org_type', 'gender']\n",    "    numerical_columns = ['student_age', 'subjects_count']\n",    "\n",    "    X_encoded = X_raw[numerical_columns].copy()\n",    "    if categorical_columns:\n",    "        encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",    "        encoded_data = encoder.fit_transform(X_raw[categorical_columns])\n",    "        feature_names = encoder.get_feature_names_out(categorical_columns)\n",    "        X_encoded = pd.concat([X_encoded, pd.DataFrame(encoded_data, columns=feature_names, index=X_raw.index)], axis=1)\n",    "    else:\n",    "        feature_names = numerical_columns\n",    "\n",    "    if X_encoded.empty or X_encoded.shape[1] == 0:\n",    "        print(f\"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª {file}, –±–æ –Ω–µ–º–∞—î –æ–∑–Ω–∞–∫ –ø—ñ—Å–ª—è –∫–æ–¥—É–≤–∞–Ω–Ω—è\")\n",    "        continue\n",    "\n",    "    X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",    "\n",    "    model = RandomForestRegressor(\n",    "        n_estimators=100,\n",    "        max_depth=10,\n",    "        min_samples_split=5,\n",    "        random_state=42\n",    "    )\n",    "    model.fit(X_train, y_train)\n",    "\n",    "    feature_importance = pd.DataFrame({\n",    "        'feature': X_encoded.columns,\n",    "        'importance': model.feature_importances_\n",    "    })\n",    "\n",    "    aggregated_importance = []\n",    "    for base_feature in required_features:\n",    "        if base_feature in numerical_columns:\n",    "            importance = feature_importance[feature_importance['feature'] == base_feature]['importance'].sum()\n",    "        else:\n",    "            importance = feature_importance[feature_importance['feature'].str.startswith(base_feature + '_')]['importance'].sum()\n",    "        aggregated_importance.append({'feature': base_feature, 'importance': importance, 'file': file.replace('.csv', '')})\n",    "    \n",    "    aggregated_importance = pd.DataFrame(aggregated_importance).sort_values('importance', ascending=False)\n",    "    print(\"\\nüîπ –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫:\")\n",    "    print(aggregated_importance[['feature', 'importance']].to_string(index=False))\n",    "\n",    "    all_importances.append(aggregated_importance)\n",    "\n",    "if all_importances:\n",    "    combined_importances = pd.concat(all_importances, ignore_index=True)\n",    "    pivot_table = combined_importances.pivot_table(\n",    "        values='importance',\n",    "        index='feature',\n",    "        columns='file',\n",    "        fill_value=0\n",    "    ).reindex(required_features)\n",    "    \n",    "    print(\"\\nüîπ –ü—ñ–¥—Å—É–º–∫–æ–≤–∞ —Ç–∞–±–ª–∏—Ü—è –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ –¥–ª—è –≤—Å—ñ—Ö —Ñ–∞–π–ª—ñ–≤:\")\n",    "    print(pivot_table.to_string())\n",    "\n",    "    plt.figure(figsize=(10, 6))\n",    "    sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='YlOrRd', cbar_kws={'label': '–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫'})\n",    "    plt.title('–¢–µ–ø–ª–æ–≤–∞ –∫–∞—Ä—Ç–∞ –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫ –¥–ª—è –≤—Å—ñ—Ö —Ñ–∞–π–ª—ñ–≤')\n",    "    plt.xlabel('–§–∞–π–ª–∏ (—Ä–æ–∫–∏)')\n",    "    plt.ylabel('–û–∑–Ω–∞–∫–∏')\n",    "    heatmap_path = os.path.join(GRAPHS_FOLDER, 'feature_importance_heatmap.png')\n",    "    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",    "    plt.close()\n",    "    print(f\"\\n‚úÖ –¢–µ–ø–ª–æ–≤–∞ –∫–∞—Ä—Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞: {heatmap_path}\")\n",    "else:\n",    "    print(\"\\n‚ö†Ô∏è –ñ–æ–¥–µ–Ω —Ñ–∞–π–ª –Ω–µ –±—É–ª–æ –æ–±—Ä–æ–±–ª–µ–Ω–æ, –ø—ñ–¥—Å—É–º–∫–æ–≤–∞ —Ç–∞–±–ª–∏—Ü—è —Ç–∞ —Ç–µ–ø–ª–æ–≤–∞ –∫–∞—Ä—Ç–∞ –Ω–µ —Å—Ç–≤–æ—Ä–µ–Ω—ñ.\")"   ],   "id": "39a26e8d41f018d3",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "",   "id": "94dd9e8609e690d5",   "outputs": [],   "execution_count": null  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 2   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython2",   "version": "2.7.6"  } }, "nbformat": 4, "nbformat_minor": 5}